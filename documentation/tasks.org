#+title: Tasks
* TASKS LIST
** TODO Understand the OTEL contract for logs :SPIKE:KUBER:
We should strive to reuse these formats, we need interoperability and creating our own agents would prove too costly.
- [ ] Check how protobuf formats are translated into arrow columns
** TODO [#A] Unit test cases of configurations :KUBER:
- [ ] Unit tests for Flight server
- [ ] Unit test for broadcast actor
- [ ] Unit test for parsing actor
- [ ] Unit test for WAL actor
- [ ] IT test for WAL writer. Tests for validating the headers on a temporary file.
- [ ]
** TODO CI setup :BALAJI:
- [ ] Create a CI script in rust for compiling the Flatbufs and Protobufs if any.
- [ ] Create a make script
** TODO [#A] Create implementations of the Actix app server :KUBER:BALAJI:DISCUSS:
- [ ] Discuss contracts for REST
- [ ] Discuss contracts for Websockets
- [ ] Discuss contracts for flights
** DONE Injest server implementation
- [X] Create a tonic server
- [X] Create a flight server
** TODO Query server implementation
- [ ] Create a health endpoint
- [ ] Create an add regex endpoint

** TODO Scripts and utils (python preferred) :BALAJI:
- [X] Script for performance testing of flight server
- [X] Script for validation of blocks
** TODO Flientbit agent
*** TODO Create a golang plugin
It should create arrow flight batches and ship it over grpc to collector
- [ ] [#A] Create a skeleton plugin
- [ ] [#A] Read and print the data
- [ ] Create a contract for http, grpc traffic.
- [ ] Create proxy interceptors in the contract.
- [ ] Create a batching function
- [ ] Side channel metadata - correlation - IP address, host id etc.
** TODO EBPF
- [ ] Self monitoring
- [ ] Metering of injest
- [ ] Blocking IPs using XDP
** TODO Add support for maps to filter data
** TODO Datafusion based SQL server
** TODO Actor models
- [ ] Actor for managing regex patterns
- [ ] Actor for parsing json and converting into arrow
- [ ] Explore *RAYON* for parallelism, we need not use actors because json parsing does not hold state.Actor for sequential writing of data
- [ ] Actor for caching the parsed arrow buffers
** TODO Try to see of the query can be called from external clients.
** TODO Check if SQLAlchemy can be made to work with the Datafusion.

** TODO Create a custom table component in Apache superset :REACT:UI:

** TODO Explore web assembly for own UI

** TODO Self healing :POC:
** TODO AI analysis
- [ ] POC on ONXX for transfering of ML models over wire
- [ ] POC for auto-encoders for anomaly detection.
- [ ] POC for convoluted auto-encoders for anomaly detection.
- [ ] ONXX runtimes in different languages.
-



* Decisions
** WAL file system
We need a write ahead log based file system for 2 reasons.
- Parquet files need to be created and pushed into iceberg schema. This must be batched to avoid small file problems.
- Logs must be durable.

** Design
+----------------------+----------------------------+-------------+
| Offset Range         | Field Name                 | Size (bytes)|
+----------------------+----------------------------+-------------+
| 0x00 - 0x07          | Magic                      | 8           |
| 0x08 - 0x0F          | Metadata Offset            | 8           |
| 0x10 - 0x11          | Metadata Length            | 2           |
| 0x12 - 0x13          | Reserved (padding)         | 2           |
| 0x14 - 0x17          | Checksum                   | 4           |
| 0x18 - 0x1F          | Reserve Offset             | 8           |
| 0x20 - 0x27          | Reserve Length             | 8           |
| 0x28 - 0x2F          | Total Block Size           | 8           |
| 0x30 - 0x3F          | Padding (for 64-byte hdr)  | 16          |
+----------------------+----------------------------+-------------+
| 0x40 - 0x40+M-1      | Metadata Bytes             | M           |
| 0x40+M - 0x40+M+D-1  | Arrow IPC Data             | D           |
| ...                  | Reserved Area (optional)   | R (optional)|
+----------------------+----------------------------+-------------+

Legend:
- `M` = metadata length (from header)
- `D` = data length (derived from file size - header - metadata - reserve)
- `R` = reserve length (from header)


** Challenges
Writes can only be sequential, and sequential writes may not be able to fully saturate the modern SSDs,
For parallelization, there are a few options.
*** Atomic offset calculations
We can calculate the total block required for a buffer and atomically store it. Next I/O request will get this offset and then add it's size
automically and save it.
However, the downside is that to calculate the size of =RecordBuffers= accurately, we need to copy the data over to a =vec![]= datastructure.
This means we have to create another copy of the data in memory, which may not be very effecient.

*** Multi-log files / dividing log files into chunks.
This approach of creating 4 log files of 1 Gb, instead of a single 4 Gb logfile, could allow the work to parallelize.
However, this approach may be more complex and needs more dilibrations. Recovery could become pretty complex.

** Considerations
- [ ] Need support for vectorized search of block headers. We can recover in parallel and create parquets in parallel using rayon.
- [ ] Need a way to parallelize writes.
- [ ] Dataloss of a few seconds may be acceptable. (Flushing for every entry in WAL could be expensive, databases flush on commit, we can flush every 2 seconds maybe)
- [ ] Check-sum and corruption checks must be done.
- [ ] Metadata should be field extractable, and no incur full serialization. (Flatbuf allows for primitive types to be de-serialized without reading entire payload)
- [ ] Need some paddings in the WAL buffer for SIMD accelerations.
- [ ] Need some padding in the headers for future explansions.
- [ ] Need schema evolution options for the Metadata
- [ ] Need validation scripts for checking block health and checksums for troubleshootings.
